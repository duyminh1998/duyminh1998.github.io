<!doctype html>
<html>
    <head>
        <title>PyCMO: Command Modern Operations Reinforcement Learning Environment</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" value="Blog explaining PyCMO, a reinforcement learning environment for Command: Modern Operations written in Python.">
        <link rel="stylesheet" href="../style.css">
    </head>
    <body>
        <main class="page-width">
            <article class="page-article">
                <header>
                    <div class="header-inner"><a id="date" href="../index.html">許實驗室</a> Minh Hua's Blog</div>
                    <nav class="menu">
                        <ul class="menu-inner">
                            <li><a href="../index.html">blog</a></li>
                            <li><a href="https://github.com/duyminh1998" target="_blank" rel="noopener noreferrer">github</a></li>
                            <li><a href="https://scholar.google.com/citations?user=dB862eQAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">google scholar</a></li>
                            <li><a href="../about/about.html">about</a></li>
                        </ul>
                    </nav>
                </header>                
                <div class="meta-info">
                    <h1>PyCMO: Command Modern Operations Reinforcement Learning Environment</h1>
                    <hr>
                    <a id="date">2-6-2023</a>
                </div>

                <div id="toc_container">
                  <h2>Contents</h2>
                  <ul class="toc_list">
                  <li><a href="#intro-head">1 Introduction</a>
                  </li>
                  <li><a href="#background-head">2 Background</a></li>
                  <ul>
                    <li><a href="#rl-env-head">2.1 Reinforcement Learning Environments</a></li>
                    <li><a href="#pysc2-head">2.2 PySC2</a></li>
                  </ul>                  
                  <li><a href="#pycmo-head">3 PyCMO</a></li>
                    <ul>
                      <li><a href="#prot-head">3.1 Protocol</a></li>
                      <li><a href="#obs-head">3.2 Observations</a></li>
                      <li><a href="#actions-head">3.3 Actions</a></li>
                      <li><a href="#env-head">3.4 Environment</a></li>
                      <li><a href="#agents-head">3.5 Agents</a></li>                      
                    </ul>
                  </li>
                  <li><a href="#conclusion-head">4 Conclusions</a></li>
                  </ul>
                </div>                
                <h2 id="intro-head">Introduction</h2>
                <div class="img-container"><img src="img/sultans_revenge_cmo_sample.gif" alt="A sample scenario in CMO." class="man-nn-img" style="width: 90%;"></div><figcaption>Figure 1. A sample scenario (Sultan's Revenge) in Command: Modern Operations.</figcaption>
                <p>This blog post chronicles the development of <a href="https://github.com/duyminh1998/pycmo" target="_blank" rel="noopener noreferrer">PyCMO</a>, a reinforcement learning environment for <a href="https://store.steampowered.com/app/1076160/Command_Modern_Operations/" target="_blank" rel="noopener noreferrer">Command: Modern Operations</a> (CMO), a warfare simulation video game that simulates military engagements and all-domain operations at the tactical level (see Figure 1). This post serves as a record of my design choices and documents my thoughts for the project's future. Although this blog post was published in February 2023, most of the work took place around August 2021. The only reason I am publishing this so late is because I have been busy with graduate school for most of 2022 (I graduated!). More on my work during that time will come in subsequent blog posts.</p>
                <p>On August 2021, in partnership with the Air Force Research Laboratory (AFRL) Information Directorate, the National Security Innovation Network (NSIN) hosted a competition that challenged contestants to develop an artificial intelligence (AI) agent capable of autonomously playing CMO. In particular, the agents were judged on their ability to complete the scenario objectives, completion time, and expenditures. The judges also evaluated the novelty and generalizability of each approach.</p>
                <p>At the time, there were several motivations that galvanized me to enter the competition. First, I was familiar with CMO from work, and I had experience developing software for CMO. For example, I had already developed a GUI to automatically generate scenario files (.scen) for design of experiments (DOE) studies, and a GUI to filter and process output data. Second, I was aware of <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii" target="_blank" rel="noopener noreferrer">AlphaStar</a>, a reinforcement learning agent that bested human players in the real-time strategy game <i>StarCraft II</i>, and I thought that most of the same that DeepMind had accomplished in <i>StarCraft II</i> (SC2) with AlphaStar could be replicated in CMO given the similarities between the two video games. The gameplay loop in both entails the movement and tasking of units to achieve objectives against opponents. Finally, my experience conducting undergraduate research on language models gave me just enough knowledge and confidence to tackle reinforcement learning (RL), a machine learning (ML) area that I had not yet explored.</p>
                <p>The preparation phase consisted of learning about AlphaStar and how the software interacted with the game; my goal was to build a version of AlphaStar compatible with CMO. Preliminary research revealed that this work would need to proceed in two phases. Whereas AlphaStar represents the game-playing agent, the environment is managed by a separate DeepMind project, the <a href="https://arxiv.org/abs/1708.04782" target="_blank" rel="noopener noreferrer">StarCraft II Learning Environment</a> (SC2LE), and the repository is hereafter referred to as <a href="https://github.com/deepmind/pysc2" target="_blank" rel="noopener noreferrer">PySC2</a>. PySC2 is a Python RL environment built around SC2 and manages the exchange of observations, actions, and rewards between the game and the agent. At this point, given the limited competition time and my one-person team's manpower, I decided to pivot my project. I prioritized the development of a reinforcement learning environment around CMO (which did not exist at the time) and left the development of the agent as a possibility only if time permits. Even if I did not get to building the agent (spoiler alert: I did not and still have not), an environment for CMO that can provide a level of abstraction akin to an <a href="https://github.com/openai/gym" target="_blank" rel="noopener noreferrer">OpenAI Gym</a> environment would be immensely helpful to the research community writ large.</p>

                <h2 id="background-head">Background</h2>
                <h3 id="rl-env-head">Reinforcement Learning Environments</h3>
                <div class="img-container"><img src="img/rl_env_loop.png" alt="Hypothetical RL loop" class="man-nn-img" style="width : 85%"></div><figcaption>Figure 2. A reinforcement learning loop.</figcaption>
                <p>The environment is an integral part of the RL loop and can be represented by a set of states, where at each timestep, the environment is in a particular state (see Figure 2). The agent interacts with the environment by performing some action, e.g. specifying the direction to move a car on a grid, directing units to attack an objective, etc. The environment processes the action and updates its state. For example, consider a 1x2 grid with a car located at the (1, 1) cell. An agent might elect to move the car to the right, and if this action is successful, the next state sees the car located at cell (1, 2). Then, the environment returns a signal comprised of the state resulting from the agent's action (called an observation) and a reward. The reward is essential to the learning process and incentivizes the agent to take favorable actions that move it closer to the goal. For example, consider the problem of training an agent to navigate through a maze. If we assigned a negative reward for each move the agent took that did not lead to the goal and a zero or positive reward for any move that makes the agent solve the maze, we have incentivized the agent to learn to solve the maze in the least number of steps.</p>
                <p>The good news with developing a warfare RL environment is that the simulation has already been built. CMO handles all the calculations like aircraft physics and missile endgames. The only thing I needed to do was extract and package the state information into a standard format, and to search for a mechanism that the agents can use to send actions to the environment. This initially sounded easy, but I discovered many challenges as the development proceeded. Although the environment is working as intended as of this writing, there are still many limitations to my approach.</p>
                
                <h3 id="pysc2-head">PySC2</h3>
                <div class="img-container"><img src="img/psyc2.png" alt="PySC2 extracting data from the game Starcraft II." class="man-nn-img" style="width: 90%;"></div><figcaption>Figure 3. A snapshot of the features that <a href="https://www.youtube.com/watch?v=-fKUyT14G-8" target="_blank" rel="noopener noreferrer">PySC2</a> extracts from <i>Starcraft II</i>.</figcaption>
                <p>There are several key PySC2 elements that I incorporated into PyCMO. PySC2 relies on the <a href="https://github.com/Blizzard/s2client-api" target="_blank" rel="noopener noreferrer">SC2 API</a>, which provides access to in-game state observation and unit control (see Figure 3). Observations are available in a wide variety of formats or "layers" including RGB pixels and structured data. PySC2's <span class="code_snippet_inline">feature.py</span> renders the feature layers from the SC2 API into numpy arrays, which are used by the agents. Modern deep learning agents that can play video games like <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="noopener noreferrer">Atari</a> are trained on pixel data, which makes that layer the most attractive. However, it is currently not possible to extract pixel data from CMO. Then, the next most natural format for observations is structured data, which are tensors of features that observe some aspect of the game. For example, the single select tensor in SC2 displays information like the unit type, health, shields, and energy for the currently selected unit. This type of data is appropriate for CMO for two reasons. First, CMO structures their data in a similar format and provides access to it through their Lua API. For example, one can call a function like <span class="code_snippet_inline">ScenEdit_GetUnit</span> to get structured data for a specific unit. The second and more important reason is that structured data is all we have in CMO, unless the developers add support for more output layers, or someone develops a tool that can extract RGB pixels from CMO.</p>
                <p>In the PySC2 documentation, the developers rightfully observed that the SC2 action space is immense with billions of possible actions. As a result, they created a set of functions that encapsulate basic and generic actions. These functions can be composed to express more complex actions, and they provide a standard format for agents to specify actions. In this sense, taking an action is the same as executing a function call within PySC2. This design also allows for the specification of actions that are only valid for certain observations. For example, one would not expect to be able to select a unit that one does not possess or is not visible. The action space is hard-coded, and the developers initially limited the available actions to actions that human players have taken. New actions would need to be added to <span class="code_snippet_inline">actions.py</span>.</p>
                <p>This design is applicable to CMO because CMO allows for game control through its Lua API. For example, one can call <span class="code_snippet_inline">ScenEdit_AttackContact</span> and pass the attacker, the contact, and several options (e.g. type of attack and weapon allocation) to direct a unit to attack an enemy contact. Thus, we can emulate PySC2's design by creating functions that encapsulate CMO actions and providing a standard format for agents to take an action. The challenge is to identify the applicable actions that should be included in the action space. Certain essential actions such as moving a unit do not have a corresponding Lua function call. In fact, the standard way to move a unit is to update its course by calling the <span class="code_snippet_inline">ScenEdit_SetUnit</span>. In these cases, creating function "wrappers" for possible actions becomes even more important.</p>
                
                <h2 id="pycmo-head">PyCMO</h2>
                <h3 id="prot-head">Protocol</h3>
                <p>To send data (in the form of Lua scripts) to the game from an external location, we rely on the TCP/IP feature of the professional edition of CMO. This is a severe limitation of the project because a professional license costs around $20,000 a year, and without the TCP/IP feature, agents have no way to send actions to the game. We will proceed with the assumption that we have access to a professional version of the game. <span class="code_snippet_inline">protocol.py</span> manages the communication between an agent and the game through a client-server model. The game acts as the server and receives and executes inputs (Lua scripts) from the client. The client is managed by the PyCMO environment. The agent decides on an action and directs the PyCMO environment to execute the action. The PyCMO environment then formats the action into the correct Lua script and uses the client to send the command to the game server.</p>
                <p>In the game's configuration file (<span class="code_snippet_inline">CPE.ini</span>), we must enable the game to listen to input (<span class="code_snippet_inline">AllowIO=1</span>). Then, there are two ways to initialize a server instance. First, we can simply launch the game through the executable and open a scenario. Note that it is important to launch scenarios in edit mode because we can only run Lua scripts in this mode. Once the scenario is opened, the game will start to listen for TCP/IP inputs (the default port is <span class="code_snippet_inline">7777</span>). Initializing the server this way is helpful for visualization purposes. However, if our intentions are to train an RL agent, then it is more appropriate to launch the game as a command line interface (CLI) instance. The <span class="code_snippet_inline">Server</span> class within <span class="code_snippet_inline">protocol.py</span> is a wrapper for a CLI instance of CMO running a specific scenario. The function <span class="code_snippet_inline">Server::start_game</span> executes a command line statement that launches an instance of the game with a scenario in CLI mode. The scenario is initially paused and the game listens for TCP/IP inputs. We can call <span class="code_snippet_inline">Server::restart</span> to start a new instance of the scenario. Thus, any RL training loop would have to rely on <span class="code_snippet_inline">Server</span>.</p>
                <p>The <span class="code_snippet_inline">Client</span> class in <span class="code_snippet_inline">protocol.py</span> relies on the <span class="code_snippet_inline">socket</span> module to connect and send data to the <span class="code_snippet_inline">Server</span> via a TCP/IP port. The most important function is <span class="code_snippet_inline">Client::send</span>, which sends data to the game's Lua API. The function does not perform any formatting of the data before sending, so outbound data must be correctly formatted before being passed to <span class="code_snippet_inline">Client::send</span> (this is taken care of by the environment, as we will detail later). When we send data, it is important to choose an encoding as there are different types that the TCP socket will accept. The type needs to be specified in the <span class="code_snippet_inline">CPE.ini</span> configuration file. </p>

                <h3 id="obs-head">Observations</h3>
                <p>At this point, we can pivot the discussion into the all-important topic of how CMO manages to extract observations from the game. There are three mechanisms that I know of that can be used to somewhat reliably output data from the game. First, one could set up a regular time trigger that uses the Lua <span class="code_snippet_inline">io</span> module to write observations to a local file. This approach is not preferable because we would need to write Lua scripts that specify the features we want to export, and this could change from scenario to scenario. This can become time-consuming and is not scalable. The second approach is to rely on one of CMO's data exporting mechanism. CMO's <span class="code_snippet_inline">Event Export</span> can perform automatic data export, but the types of features are limited to several pre-determined categories, e.g. unit positions, sensor detection attempts, fuel consumption, etc. One could conceivably use this feature to write data out to a <span class="code_snippet_inline">db3</span> or <span class="code_snippet_inline">csv</span> format and then stream the data to the environment. However, the limited feature categories hinder this approach from being perfect.</p>
                <p>For our purposes, the best approach was to rely on the Lua command <span class="code_snippet_inline">ScenEdit_ExportScenarioToXML</span>, which exports the entire scenario to an <span class="code_snippet_inline">XML</span> file. This file contains information about everything that is happening in the scenario at the time the function is invoked. In fact, one can recreate the scenario in that moment by calling the function <span class="code_snippet_inline">ScenEdit_ImportScenarioFromXML</span>.</p>
                <div class="img-container"><img src="img/sample_xml.PNG" alt="Sample XML tree of a scenario" class="man-nn-img" style="width : 85%"></div><figcaption>Figure 4. Sample XML tree of a CMO scenario.</figcaption>
                <p>The <span class="code_snippet_inline">XML</span> file is organized into a tree rooted at the <span class="code_snippet_inline">scenario</span> node, which contains no information (see Figure 4). One level below the root, the first few nodes contain "meta" information about the scenario, to include the title, description, current time, duration, etc. At this level, there are more important subtrees like <span class="code_snippet_inline">sides</span> and <span class="code_snippet_inline">activeunits</span>. The <span class="code_snippet_inline">sides</span> subtree contains information about each side, and each side has a subtree rooted at the <span class="code_snippet_inline">sides</span> node. Side information include the side's name, doctrine, reference points, contacts, and missions. Each of these features contain their own subtrees. The <span class="code_snippet_inline">activeunits</span> subtree contains information about units in the game. Each subtree within the <span class="code_snippet_inline">activeunits</span> subtree is rooted at a node that specifies its type. For example, if we had an aircraft in the game, then we can expect to see the subtree <span class="code_snippet_inline">activeunits > aircraft > {id, name, …}</span>. Each unit subtree contains information about the unit's name, type, position, as well as its sensors, mounts, and weapons. Then, within PyCMO, <span class="code_snippet_inline">features.py</span> handles the work of extracting this information from the <span class="code_snippet_inline">XML</span> file and loading them into a data format that can be used by agents.</p>
                <p>Since <span class="code_snippet_inline">XML</span> files can get unwieldy, I relied on Martin Blech's <span class="code_snippet_inline"><a href="https://github.com/martinblech/xmltodict" target="_blank" rel="noopener noreferrer">xmltodict</a></span> library to convert XML files into <span class="code_snippet_inline">JSON</span>-like (dictionary) objects. After converting the scenario <span class="code_snippet_inline">XML</span> into a dictionary, <span class="code_snippet_inline">features.py</span> parses the dictionary and packages all the relevant observations into a class called <span class="code_snippet_inline">Features</span>, which contains named tuples that collect information about the game's metadata and a side's metadata, units, mounts, loadouts, weapons, and contacts (you can read about the list of attributes in PyCMO's <a href="https://github.com/duyminh1998/pycmo/blob/main/docs/environment.md" target="_blank" rel="noopener noreferrer">documentation</a>). <span class="code_snippet_inline">Features</span> takes as input 2 required arguments: <span class="code_snippet_inline">xml</span> and <span class="code_snippet_inline">player_side</span>. <span class="code_snippet_inline">xml</span> is the path to the <span class="code_snippet_inline">XML</span> file generated from the game that contains information at a particular timestep. <span class="code_snippet_inline">player_side</span> is a string that defines the agent's side in the game. <span class="code_snippet_inline">Features</span> is an object that is unique to a particular side, so it will not hold information about other sides that a side would not usually know.</p>
                <p>There were and continues to be several challenges to parsing data from the <span class="code_snippet_inline">XML</span> file. First, each <span class="code_snippet_inline">XML</span> is unique to each scenario. Although there are overarching similarities between scenarios (e.g. one can always expect to find metadata about the scenario), certain scenarios will contain data fields that do not appear in other scenarios. For example, a scenario with ships will have ship entries, while a scenario without any ships will not. This is all to say that there was a lot of error checking that had to be done during parsing. I originally claimed that outputting data using the <span class="code_snippet_inline">io</span> feature would be less efficient because the Lua script would have to be tailored for each scenario. However, this is not completely true as there might also be future modifications (extra work) that need to be made to <span class="code_snippet_inline">features.py</span> to account for some unique scenario. I suppose it all comes down to preference. Do you want to format the data first and then output it (writing a script in Lua and then using <span class="code_snippet_inline">io</span>), or do you want to output the data first and then format it (using <span class="code_snippet_inline">ScenEdit_ExportScenarioToXML</span> and <span class="code_snippet_inline">xmltodict</span>)?</p>
                <p>The second difficulty with parsing observations is directly related to the development of agents. The purpose of <span class="code_snippet_inline">Features</span> is to encapsulate all the <i>useful</i> information about a scenario and discard the rest. However, useful does not imply relevant. Thus, the designer must still pair down <span class="code_snippet_inline">Features</span> quite a bit and tailor it for their agent. For example, if you were just training an agent to navigate an aircraft through a gauntlet of SAMs, you probably only need to know information about the aircraft and the SAMs. However, <span class="code_snippet_inline">Features</span> should have done most of the heavy lifting so that this process should be easier. A challenge that arises when one considers the possibility of employing deep learning agents in CMO is that <span class="code_snippet_inline">Features</span> is not static. There are two senses in which <span class="code_snippet_inline">Features</span> is not static. First, <span class="code_snippet_inline">Features</span> is not static because it changes depending on the scenario (as we alluded to earlier). Thus, we would need different agents for different scenarios, which precludes the possibility of one general agent that can solve a gauntlet of scenarios. Second, <span class="code_snippet_inline">Features</span> is not static <i>within</i> a scenario if there are major changes in the scenario. For example, if a unit that was initially present in the scenario dies halfway through, <span class="code_snippet_inline">Features</span> might no longer contain information about that unit. Thus, agent developers must be mindful of how to handle "missing input" when working with <span class="code_snippet_inline">Features</span>. If the agent is represented by a neural network with the input nodes being features from <span class="code_snippet_inline">Features</span>, we must know how to deal with the situation in which several input nodes receive no information because a unit gets destroyed. A thought on how to deal with this is to embed the feature/state space into common embedding, like how language models embed text using word embeddings. This would lead to a more general representation of the state space that might be more flexible to deep learning agents.</p>

                <h3 id="actions-head">Actions</h3>

                <h3 id="env-head">Environment</h3>

                <h3 id="agents-head">Agents</h3>
                
                <h2 id="conclusion-head">Conclusions</h2>                

                <div class="copyright"><a href="#">Back to top</a><br> © 2023 Minh Hua</div>             

            </article>
        </main>
    </body>
</html>
