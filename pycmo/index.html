<!doctype html>
<html>
    <head>
        <title>PyCMO: Command Modern Operations Reinforcement Learning Environment</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" value="Blog explaining PyCMO, a reinforcement learning environment for Command: Modern Operations written in Python.">
        <link rel="stylesheet" href="../style.css">
    </head>
    <body>
        <main class="page-width">
            <article class="page-article">
                <header>
                    <div class="header-inner"><a id="date" href="../index.html">許實驗室</a> Minh Hua's Blog</div>
                    <nav class="menu">
                        <ul class="menu-inner">
                            <li><a href="../index.html">blog</a></li>
                            <li><a href="https://github.com/duyminh1998" target="_blank" rel="noopener noreferrer">github</a></li>
                            <li><a href="https://scholar.google.com/citations?user=dB862eQAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">google scholar</a></li>
                            <li><a href="../about/about.html">about</a></li>
                        </ul>
                    </nav>
                </header>                
                <div class="meta-info">
                    <h1>PyCMO: Command Modern Operations Reinforcement Learning Environment</h1>
                    <hr>
                    <a id="date">2-6-2023</a>
                </div>

                <div id="toc_container">
                  <h2>Contents</h2>
                  <ul class="toc_list">
                  <li><a href="#intro-head">1 Introduction</a>
                  </li>
                  <li><a href="#background-head">2 Background</a></li>
                  <ul>
                    <li><a href="#rl-env-head">2.1 Reinforcement Learning Environments</a></li>
                    <li><a href="#pysc2-head">2.2 PySC2</a></li>
                  </ul>                  
                  <li><a href="#pycmo-head">3 PyCMO</a></li>
                    <ul>
                      <li><a href="#obs-head">3.1 Observations</a></li>
                      <li><a href="#actions-head">3.2 Actions</a></li>
                      <li><a href="#env-head">3.3 Environment</a></li>
                      <li><a href="#agents-head">3.4 Agents</a></li>                      
                    </ul>
                  </li>
                  <li><a href="#conclusion-head">4 Conclusions</a></li>
                  </ul>
                </div>                
                <h2 id="intro-head">Introduction</h2>
                <p>This blog post chronicles the development of <a href="https://github.com/duyminh1998/pycmo" target="_blank" rel="noopener noreferrer">PyCMO</a>, a reinforcement learning environment for <a href="https://store.steampowered.com/app/1076160/Command_Modern_Operations/" target="_blank" rel="noopener noreferrer">Command: Modern Operations</a> (CMO), a warfare simulation video game that simulates military engagements and all-domain operations at the tactical level. This post serves as a record of my design choices and documents my thoughts for the project's future. Although this blog post was published in February 2023, most of the work took place around August 2021. The only reason I am publishing this so late is because I have been busy with graduate school for most of 2022 (I graduated!). More on my work during that time will come in subsequent blog posts.</p>
                <p>On August 2021, in partnership with the Air Force Research Laboratory (AFRL) Information Directorate, the National Security Innovation Network (NSIN) hosted a competition that challenged contestants to develop an artificial intelligence (AI) agent capable of autonomously playing CMO. In particular, the agents were judged on their ability to complete the scenario objectives, completion time, and expenditures. The judges also evaluated the novelty and generalizability of each approach.</p>
                <p>At the time, there were several motivations that galvanized me to enter the competition. First, I was familiar with CMO from work, and I had experience developing software for CMO. For example, I had already developed a GUI to automatically generate scenario files (.scen) for design of experiments (DOE) studies, and a GUI to filter and process output data. Second, I was aware of <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii" target="_blank" rel="noopener noreferrer">AlphaStar</a>, a reinforcement learning agent that bested human players in the real-time strategy game <i>StarCraft II</i>, and I thought that most of the same that DeepMind had accomplished in <i>StarCraft II</i> (SC2) with AlphaStar could be replicated in CMO given the similarities between the two video games. The gameplay loop in both entails the movement and tasking of units to achieve objectives against opponents. Finally, my experience conducting undergraduate research on language models gave me just enough knowledge and confidence to tackle reinforcement learning (RL), a machine learning (ML) area that I had not yet explored.</p>
                <p>The preparation phase consisted of learning about AlphaStar and how the software interacted with the game; my goal was to build a version of AlphaStar compatible with CMO. Preliminary research revealed that this work would need to proceed in two phases. Whereas AlphaStar represents the game-playing agent, the environment is managed by a separate DeepMind project, the <a href="https://arxiv.org/abs/1708.04782" target="_blank" rel="noopener noreferrer">StarCraft II Learning Environment</a> (SC2LE), and the repository is hereafter referred to as <a href="https://github.com/deepmind/pysc2" target="_blank" rel="noopener noreferrer">PySC2</a>. PySC2 is a Python RL environment built around SC2 and manages the exchange of observations, actions, and rewards between the game and the agent. At this point, given the limited competition time and my one-person team's manpower, I decided to pivot my project. I prioritized the development of a reinforcement learning environment around CMO (which did not exist at the time) and left the development of the agent as a possibility only if time permits. Even if I did not get to building the agent (spoiler alert: I did not and still have not), an environment for CMO that can provide a level of abstraction akin to an <a href="https://github.com/openai/gym" target="_blank" rel="noopener noreferrer">OpenAI Gym</a> environment would be immensely helpful to the research community writ large.</p>

                <h2 id="background-head">Background</h2>
                <h3 id="rl-env-head">Reinforcement Learning Environments</h3>
                <p>The environment is an integral part of the RL loop and can be represented by a set of states, where at each timestep, the environment is in a particular state. The agent interacts with the environment by performing some action, e.g. specifying the direction to move a caron a grid, directing units to attack an objective, etc. The environment processes the action and updates its state. For example, consider a 1x2 grid with a car located at the (1, 1) cell. An agent might elect to move the car to the right, and if this action is successful, the next state sees the car located at cell (1, 2). Then, the environment returns a signal comprised of the state resulting from the agent's action (called an observation) and a reward. The reward is essential to the learning process and incentivizes the agent to take favorable actions that move it closer to the goal. For example, consider the problem of training an agent to navigate through a maze. If we assigned a negative reward for each move the agent took that did not lead to the goal and a zero or positive reward for any move that makes the agent solve the maze, we have incentivized the agent to learn to solve the maze in the least number of steps.</p>
                <p>The good news with developing a warfare RL environment is that the simulation has already been built. CMO handles all the calculations like platform physics and missile endgames. The only thing I needed to do was extract and package the state information into a standard format, and to search for a mechanism that the agents can use to send actions to the environment. This initially sounded easy, but I discovered many challenges as the development proceeded. Although the environment is working as intended as of this writing, there are still many limitations to my approach.</p>
                
                <h3 id="pysc2-head">PySC2</h3>
                <p>There are several key PySC2 elements that I incorporated into PyCMO. PySC2 relies on the <a href="https://github.com/Blizzard/s2client-api" target="_blank" rel="noopener noreferrer">SC2 API</a>, which provides access to in-game state observation and unit control. Observations are available in a wide variety of formats or "layers" including RGB pixels and structured data. PySC2's <span class="code_snippet_inline">feature.py</span> renders the feature layers from the SC2 API into numpy arrays, which are used by the agents. Modern deep learning agents that can play video games like <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="noopener noreferrer">Atari</a> are trained on pixel data, which makes that layer the most attractive. However, it is currently not possible to extract pixel data from CMO. Then, the next most natural format for observations is structured data, which are tensors of features that observe some aspect of the game. For example, the single select tensor in SC2 displays information like the unit type, health, shields, and energy for the currently selected unit. This type of data is appropriate for CMO for two reasons. First, CMO structures their data in a similar format and provides access to it through their Lua API. For example, one can call a function like <span class="code_snippet_inline">ScenEdit_GetUnit</span> to get structured data for a specific unit. The second and more important reason is that structured data is all we have in CMO, unless the developers add support for more output layers, or someone develops a tool that can extract RGB pixels from CMO.</p>
                <p>In the PySC2 documentation, the developers rightfully observed that the SC2 action space is immense with billions of possible actions. As a result, they created a set of functions that encapsulate basic and generic actions. These functions can be composed to express more complex actions, and they provide a standard format for agents to specify actions. In this sense, taking an action is the same as executing a function call within PySC2. This design also allows for the specification of actions that are only valid for certain observations. For example, one would not expect to be able to select a unit that one does not possess or is not visible. The action space is hard-coded, and the developers initially limited the available actions to actions that human players have taken. New actions would need to be added to <span class="code_snippet_inline">actions.py</span>.</p>
                <p>This design is applicable to CMO because CMO allows for game control through its Lua API. For example, one can call <span class="code_snippet_inline">ScenEdit_AttackContact</span> and pass the attacker, the contact, and several options (e.g. type of attack and weapon allocation) to direct a unit to attack an enemy contact. Thus, we can emulate PySC2's design by creating functions that encapsulate CMO actions and providing a standard format for agents to take an action. The challenge is to identify the applicable actions that should be included in the action space. Certain essential actions such as moving a unit do not have a corresponding Lua function call. In fact, the standard way to move a unit is to update its course by calling the <span class="code_snippet_inline">ScenEdit_SetUnit</span>. In these cases, creating function "wrappers" for possible actions becomes even more important.</p>
                
                <h2 id="pycmo-head">PyCMO</h2>
                <h3 id="obs-head">Observations</h3>

                <h3 id="actions-head">Actions</h3>

                <h3 id="env-head">Environment</h3>

                <h3 id="agents-head">Agents</h3>
                
                <h2 id="conclusion-head">Conclusions</h2>                

                <div class="copyright"><a href="#">Back to top</a><br> © 2023 Minh Hua</div>             

            </article>
        </main>
    </body>
</html>
